---
title: "Kernel estimation in regression on vector and function spaces"
subtitle: "Sid Kankanala, Marcia Schafgans, and Victoria Zinde-Walsh"
author: Comments by Jeffrey Racine @ CESG 2023
date: today
format:
  revealjs:
    background-transition: fade
    center: true
    css: custom.css
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            tex: {
              tags: 'ams'
            }
          };
          </script>
editor: visual
---

## Project Overview

-   The authors consider kernel-weighted nonparametric regression when the distribution of the regressors may not possess absolute continuity with respect to the Lebesgue measure

-   They demonstrate that convergence rates are influenced non-trivially by the underlying distribution

-   In the case of absolutely continuous measures, their approach weakens the usual regularity conditions

-   They extend their analysis to encompass kernel regression with multiple functional regressors

## Setting

-   The authors consider a smooth nonparametric model with additive errors $$Y=m(X)+u,\qquad E(u|X)=0$$

-   $X$ may belong to a function space, e.g., continuous functions on a compact set, or a more general metric space

-   $X$ does not necessarily have to be a vector or a function

-   $X$ is supported on some domain in a vector or metric, semi-metric space denoted as $\Xi$

-   The authors focus is on the generality of assumptions on the underlying distribution of $X$, i.e., on $F_X(x)$

::: notes
The fourteenth letter in the Greek alphabet, $\Xi$ is pronounced "kai"
:::

## Setting Cont.

-   The authors consider three cases:

    i.  $\Xi=\mathbb{R}^q$ (standard kernel regression)

    ii. $\Xi=\Xi^1$ (univariate metric space, i.e., functional regression)

    iii. $\Xi=\Xi^q=\Xi^1\times\cdots\times\Xi^1$ (product metric space)

-   The authors contribute to the literature by:

    i.  Establishing a novel CLT

    ii. Demonstrating that convergence rates are affected by the underlying distribution

    iii. Demonstrating that they can weaken usual regularity conditions assumed for such analysis

## Approach

```{r}
fx <- function(x) {
  0.5*dnorm(x+.767)+3*dnorm((x+.767-0.8)/.1)+2*dnorm((x+.767-1.2)/.1) 
}

fxmsd <- function(x) {
  0.5*dnorm(x,mean=-.767)+3*.1*dnorm(x,mean=-(.767-0.8),sd=.1)+2*.1*dnorm(x,mean=-(.767-1.2),sd=.1) 
}

rfxmsd <- function(n) {
  P <- sample(c("A","B","C"),n,replace=TRUE,prob=c(0.5,0.3,0.2))
  P.n <- numeric(n)
  P.n[P=="A"] <- rnorm(length(P[P=="A"]),mean=-.767)
  P.n[P=="B"] <- rnorm(length(P[P=="B"]),mean=-(.767-0.8),sd=.1)
  P.n[P=="C"] <- rnorm(length(P[P=="C"]),mean=-(.767-1.2),sd=.1) 
  return(P.n)
}

x <- seq(-3.5,2,length=1000)

par(mfrow=c(2,2))

plot(x,fx(x),main="Density",xlab="X",ylab="fx(x)",type="l")
lines(x,fxmsd(x),lty=2,col=2)

n <- 10^4
breaks <- 100

X <- sort(rfxmsd(n))

ylim <- range(hist(X,breaks=breaks,plot=FALSE)$density,fxmsd(x))

hist(X,xlim=c(-3.5,2),ylim=ylim,breaks=breaks,prob=TRUE,
     main="Histogram/Density",
     sub="Random Draw from fx(x)")
lines(x,fxmsd(x))

dgp <- sin(2.5*X)
Y <- dgp + rnorm(n,sd=sd(dgp))
plot(X,Y,cex=.1,col="grey")
ghat.dgp <- lm(Y~sin(2.5*X))
lines(X,fitted(ghat.dgp),col=2,lwd=2)
library(np)
options(np.messages=FALSE,np.tree=TRUE)
ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
lines(X,fitted(ghat.nw),col=3,lwd=3)
```

## Comments

-   Figures - mixing $f(x)$ on vertical axis and $F(x)$ in description?
