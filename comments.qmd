---
title: "Kernel estimation in regression on vector and function spaces"
subtitle: "Sid Kankanala, Marcia Schafgans, and Victoria Zinde-Walsh"
author: Comments by Jeffrey Racine @ CESG 2023
date: today
format:
  revealjs:
    background-transition: fade
    center: true
    css: custom.css
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            tex: {
              tags: 'ams'
            }
          };
          </script>
knitr:
  opts_chunk:
    autodep: true
    collapse: true
    cache: true
    echo: false
    eval.after: "fig.cap"
    fig.align: "center"
    message: false
    warning: false
    R.options:
      np.messages: false
      plot.par.mfrow: false
bibliography: comments.bib      
editor: visual
---

```{r global_options}
#| include: false
library(np)
set.seed(42)
```

## Project Overview

-   The authors consider kernel-weighted nonparametric regression when the distribution of the regressors may not possess absolute continuity with respect to the Lebesgue measure

-   They demonstrate that convergence rates are influenced non-trivially by the underlying distribution

-   In the case of absolutely continuous measures, their approach weakens the usual regularity conditions

-   They extend their analysis to encompass kernel regression with multiple functional regressors

## Setting

-   The authors consider a smooth nonparametric model with additive errors $$Y=m(X)+u,\qquad E(u|X)=0$$

-   $X$ may belong to a function space, e.g., continuous functions on a compact set, or a more general metric space

-   $X$ does not necessarily have to be a vector or a function

-   $X$ is supported on some domain in a vector or metric, semi-metric space denoted as $\Xi$

-   The authors focus is on the generality of assumptions on the underlying distribution of $X$, i.e., on $F_X(x)$

::: notes
The fourteenth letter in the Greek alphabet, $\Xi$ is pronounced "kai"
:::

## Setting Cont.

-   The authors consider three cases:

    i.  $\Xi=\mathbb{R}^q$ (standard kernel regression)

    ii. $\Xi=\Xi^1$ (univariate metric space, i.e., functional regression)

    iii. $\Xi=\Xi^q=\Xi^1\times\cdots\times\Xi^1$ (product metric space)

-   The authors contribute to the literature by:

    i.  Establishing a novel CLT

    ii. Demonstrating that convergence rates are affected by the underlying distribution

    iii. Demonstrating that they can weaken usual regularity conditions assumed for such analysis

## What do we currently know ($X\in\mathbb{D}^1$)?

-   When $X$ has discrete support, kernel smoothing can attain a $\sqrt{n}$ rate for $E(Y|X=x)$

-   This is achieved by using, e.g., cross-validation to select smoothing parameters when using *categorical* kernel functions [@LI_RACINE:2007] or *continuous* kernel functions [@BIERENS:1983]

    ```{r}
    n <- 100
    X <- sort(rbinom(n,5,.3))
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n,sd=sd(dgp))
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~sin(2.5*X))
    points(X,fitted(ghat.dgp),col=2)
    library(np)
    options(np.messages=FALSE,np.tree=TRUE)
    ghat.nw <- npreg(Y~ordered(X),bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

## What do we currently know ($X\in\mathbb{R}^1$)?

-   When $X$ has continuous support by is *unrelated* to $Y$, kernel smoothing can attain a $\sqrt{n}$ rate for $E(Y|X=x)=E(Y)$

-   This can be attained when using, e.g., cross-validation to select bandwidths that select $h\to\infty$ with high probability when using *continuous* kernel functions [@HALL_LI_RACINE:2007]

    ```{r}
    n <- 100
    X <- sort(runif(n))
    dgp <- rep(1,n)
    Y <- dgp + rnorm(n)
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~1)
    points(X,fitted(ghat.dgp),col=2)
    library(np)
    options(np.messages=FALSE,np.tree=TRUE)
    ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

## What do we currently know ($X\in\mathbb{R}^1$)?

-   Otherwise, when kernel smoothing under common regularity conditions, we attain a $\sqrt{nh}$ rate of convergence

-   The authors consider, instead, cases where the convergence rate, when using *continuous* kernel functions

## Approach

```{r}
fx <- function(x) {
  0.5*dnorm(x+.767)+3*dnorm((x+.767-0.8)/.1)+2*dnorm((x+.767-1.2)/.1) 
}

fxmsd <- function(x) {
  0.5*dnorm(x,mean=-.767)+3*.1*dnorm(x,mean=-(.767-0.8),sd=.1)+2*.1*dnorm(x,mean=-(.767-1.2),sd=.1) 
}

rfxmsd <- function(n) {
  P <- sample(c("A","B","C"),n,replace=TRUE,prob=c(0.5,0.3,0.2))
  P.n <- numeric(n)
  P.n[P=="A"] <- rnorm(length(P[P=="A"]),mean=-.767)
  P.n[P=="B"] <- rnorm(length(P[P=="B"]),mean=-(.767-0.8),sd=.1)
  P.n[P=="C"] <- rnorm(length(P[P=="C"]),mean=-(.767-1.2),sd=.1) 
  return(P.n)
}

x <- seq(-3.5,2,length=1000)

par(mfrow=c(2,2))

plot(x,fx(x),main="Density",xlab="X",ylab="fx(x)",type="l")
lines(x,fxmsd(x),lty=2,col=2)

n <- 10^4
breaks <- 100

X <- sort(rfxmsd(n))

ylim <- range(hist(X,breaks=breaks,plot=FALSE)$density,fxmsd(x))

hist(X,xlim=c(-3.5,2),ylim=ylim,breaks=breaks,prob=TRUE,
     main="Histogram/Density",
     sub="Random Draw from fx(x)")
lines(x,fxmsd(x))

dgp <- sin(2.5*X)
Y <- dgp + rnorm(n,sd=sd(dgp))
plot(X,Y,cex=.1,col="grey")
ghat.dgp <- lm(Y~sin(2.5*X))
lines(X,fitted(ghat.dgp),col=2,lwd=2)
library(np)
options(np.messages=FALSE,np.tree=TRUE)
ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
lines(X,fitted(ghat.nw),col=3,lwd=3)
```

## Comments

-   Figures - mixing $f(x)$ on vertical axis and $F(x)$ in description?
