---
title: "Kernel estimation in regression on vector and function spaces"
subtitle: "Sid Kankanala, Marcia Schafgans, and Victoria Zinde-Walsh"
author: Comments by Jeffrey Racine @ CESG 2023
date: today
format:
  revealjs:
    background-transition: fade
    center: true
    css: custom.css
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            tex: {
              tags: 'ams'
            }
          };
          </script>
knitr:
  opts_chunk:
    autodep: true
    collapse: true
    cache: true
    echo: false
    eval.after: "fig.cap"
    fig.align: "center"
    message: false
    warning: false
    R.options:
      np.messages: false
      plot.par.mfrow: false
bibliography: comments.bib      
editor: visual
---

```{r global_options}
#| include: false
library(np)
set.seed(42)
```

## Project Overview

-   The authors consider kernel-weighted nonparametric regression when the distribution of the regressors may not possess absolute continuity with respect to the Lebesgue measure

-   They demonstrate that convergence rates are influenced non-trivially by the underlying distribution

-   In the case of absolutely continuous measures, their approach weakens the usual regularity conditions

-   They extend their analysis to encompass kernel regression with multiple functional regressors

## Setting

-   The authors consider a smooth nonparametric model with additive errors $$Y=m(X)+u,\qquad E(u|X)=0$$

-   $X$ may belong to a function space, e.g., continuous functions on a compact set, or a more general metric space

-   $X$ does not necessarily have to be a vector or a function

-   $X$ is supported on some domain in a vector or metric, semi-metric space denoted as $\Xi$

-   The authors focus is on the generality of assumptions on the underlying distribution of $X$, i.e., on $F_X(x)$

::: notes
The fourteenth letter in the Greek alphabet, $\Xi$ is pronounced "kai"
:::

## Setting Cont.

-   The authors consider three cases:

    i.  $\Xi=\mathbb{R}^q$ (standard kernel regression)

    ii. $\Xi=\Xi^1$ (univariate metric space, i.e., functional regression)

    iii. $\Xi=\Xi^q=\Xi^1\times\cdots\times\Xi^1$ (product metric space)

-   The authors contribute to the literature by:

    i.  Establishing a novel CLT

    ii. Demonstrating that convergence rates are affected by the underlying distribution

    iii. Demonstrating that they can weaken usual regularity conditions assumed for such analysis

## What do we currently know ($X\in\mathbb{D}^1$)?

-   When $X$ has discrete support, kernel smoothing can attain a $\sqrt{n}$ rate for $E(Y|X=x)$

-   This is achieved by using, e.g., cross-validation to select smoothing parameters when using *categorical* kernel functions [@LI_RACINE:2007] or *continuous* kernel functions [@BIERENS:1983]

    ```{r categoricalkernel}
    n <- 100
    X <- sort(rbinom(n,5,.3))
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n,sd=sd(dgp))
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~sin(2.5*X))
    points(X,fitted(ghat.dgp),col=2)
    library(np)
    options(np.messages=FALSE,np.tree=TRUE)
    ghat.nw <- npreg(Y~ordered(X),bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

## What do we currently know ($X\in\mathbb{R}^1$)?

-   When $X$ has continuous support by is *unrelated* to $Y$, kernel smoothing can attain a $\sqrt{n}$ rate for $E(Y|X=x)=E(Y)$

-   This can be attained when using, e.g., cross-validation to select bandwidths that select $h\to\infty$ with high probability when using *continuous* kernel functions [@HALL_LI_RACINE:2007]

    ```{r continuouskernel}
    n <- 100
    X <- sort(runif(n))
    dgp <- rep(1,n)
    Y <- dgp + rnorm(n)
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~1)
    points(X,fitted(ghat.dgp),col=2)
    library(np)
    options(np.messages=FALSE,np.tree=TRUE)
    ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

## What do we currently know ($X\in\mathbb{R}^1$)?

-   Otherwise, when kernel smoothing under common regularity conditions, we attain a $\sqrt{nh}$ rate of convergence

-   The authors consider, instead, cases where the convergence rate, when using *continuous* kernel functions

## Assessing Pointwise Rates ($L^2$) {.scrollable}

### Catgorical $X\in\mathbb{D}^1$

```{r ordered}
## Write a monte carlo simulation that compute the pointwise MSE of the fitted
## values generated by npreg() using the newdata evaluation points
library(robustbase)
library(np)
options(np.tree=TRUE,np.messages=FALSE)

M <- 1000
n.trials <- 2
n.vec <- c(50,100,200,400,800,1600,3200)
X.pred.dgp <- X.pred <- 0:2
X.pred <- ordered(X.pred)
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- sort(rbinom(n.vec[j],n.trials,.5))
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~ordered(X,levels=0:n.trials))
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise MSE at each support point

mse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
mse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))

for(j in 1:length(n.vec)) {
  mse.dgp[j,] <- colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2)
  mse[j,] <- colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2)
}

mse.dgp.pointwise <- numeric()
mse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  mse.dgp.pointwise[i] <- coef(ltsReg(log(mse.dgp[,i])~log(n.vec)))[2]
  mse.pointwise[i] <- coef(ltsReg(log(mse[,i])~log(n.vec)))[2]
}
foo.ot <- data.frame(X.pred,mse.dgp.pointwise,mse.pointwise,mse.pointwise/mse.dgp.pointwise)
colnames(foo.ot) <- c("X","Oracle","NW","ratio")
```

```{r orderedtable}
knitr::kable(foo.ot,digits=2,align="rlll")
```

### Irrelevant $X\in\mathbb{R}^1$

```{r irrelevant}
## Write a monte carlo simulation that compute the pointwise MSE of the fitted
## values generated by npreg() using the newdata evaluation points
## X.pred = c(-2,0,2) for a sample size of n = 50, 100, 200, 400, 800.

library(np)
library(robustbase)
options(np.tree=TRUE,np.messages=FALSE)
set.seed(42)

M <- 1000
n.vec <- c(50,100,200,400,800,1600)
X.pred.dgp <- X.pred <- c(-2,0,2)
X.pred <- X.pred
dgp.pred <- rep(0,length(X.pred))
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- runif(n.vec[j],min=-3,max=3)
    dgp <- rep(0,length=n.vec[j])
    Y <- dgp + rnorm(n.vec[j],sd=.1)
    ghat.dgp <- lm(Y~1)
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise MSE at each support point

mse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
mse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  mse.dgp[j,] <- colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2)
  mse[j,] <- colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2)
}

mse.dgp.pointwise <- numeric()
mse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  mse.dgp.pointwise[i] <- coef(ltsReg(log(mse.dgp[,i])~log(n.vec)))[2]
  mse.pointwise[i] <- coef(ltsReg(log(mse[,i])~log(n.vec)))[2]
}
foo.it <- data.frame(X.pred,mse.dgp.pointwise,mse.pointwise,mse.pointwise/mse.dgp.pointwise)
colnames(foo.it) <- c("X","Oracle","NW","ratio")
```

```{r irrelevanttable}
knitr::kable(foo.it,digits=2,align="rlll")
```

### Relevant $X\in\mathbb{R}^1$

```{r continuous}
## Write a monte carlo simulation that compute the pointwise MSE of the fitted
## values generated by npreg() using the newdata evaluation points
## X.pred = c(-2,0,2) for a sample size of n = 50, 100, 200, 400, 800.

library(np)
library(robustbase)
options(np.tree=TRUE,np.messages=FALSE)
set.seed(42)

M <- 1000
n.vec <- c(100,200,400,800,1600,3200,6400,12800)
X.pred.dgp <- X.pred <- c(-2,0,2)
X.pred <- X.pred
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- runif(n.vec[j],min=-3,max=3)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise MSE at each support point

mse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
mse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  mse.dgp[j,] <- colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2)
  mse[j,] <- colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2)
}

mse.dgp.pointwise <- numeric()
mse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  mse.dgp.pointwise[i] <- coef(ltsReg(log(mse.dgp[,i])~log(n.vec)))[2]
  mse.pointwise[i] <- coef(ltsReg(log(mse[,i])~log(n.vec)))[2]
}
foo.ct <- data.frame(X.pred,mse.dgp.pointwise,mse.pointwise,mse.pointwise/mse.dgp.pointwise)
colnames(foo.ct) <- c("X","Oracle","NW","ratio")
```

```{r continuoustable}
knitr::kable(foo.ct,digits=2,align="rlll")
```

## Comments

-   Figures - mixing $f(x)$ on vertical axis and $F(x)$ in description?
