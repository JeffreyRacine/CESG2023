---
title: "Kernel estimation in regression on vector and function spaces"
subtitle: "Sid Kankanala, Marcia Schafgans, and Victoria Zinde-Walsh"
author: Comments by Jeffrey Racine @ CESG 2023
date: today
format:
  revealjs:
    background-transition: fade
    center: true
    chalkboard: true    
    css: custom.css
    footer: "<a href='https://jeffreyracine.github.io/CESG2023'>jeffreyracine.github.io/CESG2023</a> | <a href='https://jeffreyracine-github-io.translate.goog/CESG2023/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp#/title-slide'>Translate</a>"
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            tex: {
              tags: 'ams'
            }
          };
          </script>
    incremental: false
    link-external-newwindow: true
    ## multiplex: true will create two html files:
    ## 1. index.html: This is the file you should publish online and that your
    ## audience should view.
    ## 2. index-speaker.html: This is the file that you should present from.
    ## This file can remain on your computer and does not need to be published
    ## elsewhere.
    multiplex: true
    preview-links: auto
    self-contained-math: false
    show-notes: false
    slide-number: true
    theme: default
    touch: true
    transition: slide
knitr:
  opts_chunk:
    autodep: true
    collapse: true
    cache: true
    echo: false
    eval.after: "fig.cap"
    fig.align: "center"
    message: false
    warning: false
    R.options:
      np.messages: false
      plot.par.mfrow: false
bibliography: comments.bib      
editor: visual
---

```{r global_options}
#| include: false
library(robustbase)
library(np)
options(np.tree=TRUE,np.messages=FALSE)
M <- 1000
```

## Slide Pro-Tips {.smaller}

::: nonincremental
-   Link to slides (case sensitive) <a href="https://jeffreyracine.github.io/CESG2023">jeffreyracine.github.io/CESG2023</a>, <a href="https://jeffreyracine-github-io.translate.goog/CESG2023/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp#/title-slide">Translate slides</a>

-   View **full screen** by pressing the F key (press the Esc key to revert)

-   Access **navigation menu** by pressing the M key (click X in navigation menu to close)

-   **Advance** using arrow keys

-   **Zoom** in by holding down the Alt key in MS Windows, Opt key in macOS or Ctrl key in Linux, and clicking on any screen element (Alt/Opt/Ctrl click again to zoom out)

-   **Export to a PDF** by pressing the E key (wait a few seconds, then print \[or print using system dialog\], enable landscape layout, then save as PDF - press the E key to revert)

-   Enable drawing tools - chalk **board** by pressing the B key (B to revert), notes **canvas** by pressing the C key (C to revert), press the Del key to erase, press the D key to **download drawings**
:::

::: notes
Encourage participants to print/save a PDF copy of the slides as there is no guarantee that this material will be there when they realize it might be useful
:::

## Project Overview

-   The authors consider kernel-weighted nonparametric regression when the distribution of the regressors may not possess absolute continuity with respect to the Lebesgue measure

-   They demonstrate that convergence rates are influenced non-trivially by the underlying distribution

-   In the case of absolutely continuous measures, their approach weakens the usual regularity conditions

-   They extend their analysis to encompass kernel regression with multiple functional regressors

## Setting

-   The authors consider a smooth nonparametric model with additive errors $$Y=m(X)+u,\qquad E(u|X)=0$$

-   $X$ may belong to a function space, e.g., continuous functions on a compact set, or a more general metric space

-   $X$ does not necessarily have to be a vector or a function

-   $X$ is supported on some domain in a vector or metric, semi-metric space denoted as $\Xi$

-   The authors focus is on the generality of assumptions on the underlying distribution of $X$, i.e., on $F_X(x)$

::: notes
The fourteenth letter in the Greek alphabet, $\Xi$ is pronounced "kai"
:::

## Setting Cont.

-   The authors consider three cases for the Nadaraya-Watson estimator:

    i.  $\Xi=\mathbb{R}^q$ (standard kernel regression)

    ii. $\Xi=\Xi^1$ (univariate metric space, i.e., functional regression)

    iii. $\Xi=\Xi^q=\Xi^1\times\cdots\times\Xi^1$ (product metric space)

-   The authors contribute to the literature by:

    i.  Establishing a novel CLT

    ii. Demonstrating that convergence rates are affected by the underlying distribution

    iii. Demonstrating that they can weaken usual regularity conditions assumed for such analysis

## What do we know ($X\in\mathbb{D}^1$)?

-   When $X$ has discrete support, kernel smoothing via the Nadaraya-Watson estimator can attain a $\sqrt{n}$ rate for $E(Y|X=x)$

-   This is achieved by using, e.g., cross-validation to select smoothing parameters when using *categorical* kernel functions [@LI_OUYANG_RACINE:2009] or *continuous* kernel functions [@BIERENS:1983]

    ```{r categoricalkernel}
    #| out.width = "75%"
    library(crs)
    options(crs.messages=FALSE)
    set.seed(42)
    n <- 100
    X <- sort(rbinom(n,5,.3))
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n,sd=sd(dgp))
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~sin(2.5*X))
    points(X,fitted(ghat.dgp),col=2)
    ghat.nw <- npreg(Y~ordered(X),bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

::: notes
The authors consider point mass, i.e., discrete support, distributions for $X$
:::

## What do we know ($X\in\mathbb{R}^1$)?

-   When $X$ has continuous support but is *unrelated* to $Y$, kernel smoothing via the Nadaraya-Watson estimator can attain a $\sqrt{n}$ rate for $E(Y|X=x)=E(Y)$

-   This can be attained when using, e.g., cross-validation to select bandwidths that select $h\to\infty$ with high probability as $n\to\infty$ when using *continuous* kernel functions [@HALL_LI_RACINE:2007]

    ```{r continuouskernel}
    #| out.width = "75%"
    set.seed(42)    
    n <- 1000
    X <- sort(runif(n))
    dgp <- rep(1,n)
    Y <- dgp + rnorm(n)
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~1)
    lines(X,fitted(ghat.dgp),col=2,lty=1)
    ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
    lines(X,fitted(ghat.nw),col=3,lty=2,lwd=2)
    lines(X,dgp,col=4,lty=3,lwd=3)
    legend("topleft",c("Oracle","NP","DGP"),lty=1:3,lwd=c(1,2,2),col=2:4,bty="n")
    ```

::: notes
The authors consider continuous support distributions for $X$
:::

## What do we know ($X\in\mathbb{R}^1$)?

-   When $X$ has continuous support, kernel smoothing via the Generalized Local Polynomial estimator can attain a $\sqrt{n}$ rate for $E(Y|X=x)=E(Y)$

-   This can be attained when using, e.g., cross-validation to select the bandwidth *and* polynomial order when using *continuous* kernel functions [@HALL_RACINE:2015] with nonstandard bandwidth behaviour (i.e., $h\to\infty$ as $n\to\infty$)

    ```{r continuouskernelglp}
    #| out.width = "75%"
    library(crs)
    options(crs.messages=FALSE)
    set.seed(42)    
    n <- 1000
    X <- sort(runif(n))
    dgp <- sin(2*pi*X)
    Y <- dgp + rnorm(n,sd=.5*sd(dgp))
    plot(X,Y,cex=.25,col="grey")
    ghat.dgp <- lm(Y~dgp)
    lines(X,fitted(ghat.dgp),col=2,lty=1)
    ghat.glp <- npglpreg(Y~X,ckertype="epanechnikov")
    lines(X,fitted(ghat.glp),col=3,lty=2,lwd=2)
    lines(X,dgp,col=4,lty=3,lwd=2)
    legend("topleft",c("Oracle","NP-GLP","DGP"),lty=1:3,lwd=c(1,2,2),col=2:4,bty="n")
    ```

::: notes
The authors consider only the local constant estimator where $h\to0$ as $n\to\infty$, and this estimator behaves well for regression, but perhaps rate improvements could be achieved by using this estimator with higher order polynomials
:::

## What do we know ($X\in\mathbb{R}^1$)?

-   Otherwise, when kernel smoothing with the Nadaraya-Watson estimator under common regularity conditions, we attain a $\sqrt{nh}$ rate of convergence (the same holds true for local polynomial estimators with ad hoc fixed order $p$)

-   This is the case the authors consider (simulations use $p=0$, i.e., the Nadaraya-Watson or "local constant" estimator)

-   Using an appropriate bandwidth, for $X\in\mathbb{R}^1$, it is well-established that ${\text RMSE}\propto n^{-2/5}$ using second-order kernel functions

-   The authors consider, instead, cases where the convergence rate, when using *continuous* kernel functions can be faster than the usual rate, for instance when there exist mass points in the support of $X$

## Assessing Pointwise Rates ($L^2$)

-   Bandwidths are "global" rather than "pointwise" ("fixed" and IMSE-optimal)

-   Is a fixed bandwidth the best in this setting? IMSE-optimal appropriate?

-   What about "locally adaptive" bandwidths?

-   The simulations are extensive but might be improved

## Assessing Pointwise Rates ($L^2$)

-   I am not sure I learn much about *rates* from the simulations

    -   The pointwise bias for the local constant estimator of $g(x)=E(Y|X=x)$ is $\frac{h^2}{2f(x)}\left(2g'(x)f'(x)+g^{''}(x)f(x)\right)\kappa_2$  and $h^2\left(\frac{1}{2}g^{''}(x)\right)\kappa_2$ for the local linear estimator

    -   They present figures, for a given $n$, that reveal how *pointwise* RMSE varies over $X$

    -   I would be more interested in how *pointwise* RMSE varies over $n$ (i.e., the rate of convergence)


## Assessing Pointwise Rates ($L^2$)

-   Perhaps the following might be an alternative to the existing simulations

    -   run Monte Carlo simulations comparing various methods on predictions at a set of support points (as is currently done by the authors)

    -   consider a range of sample sizes

    -   for each Monte Carlo replication record the pointwise RMSE

    -   finally, regress $\log({\text RMSE})$ on $\log(n)$ and record the coefficient on $\log(n)$

-   The coefficient on $\log(n)$ will be the empirical rate of convergence

-   For example, if ${\text RMSE} \propto n^{-2/5}$ then $\log({\text RMSE}) =\alpha+\beta\log(n)$ and $\beta$ can be estimated and should be close to $-0.40$

## Categorical Kernel, Relevant $X\in\mathbb{D}^1$

We consider $Y=\sin(2X)+\epsilon$, where $X\{0,1,2\}$

We implement the "Oracle" estimator and the kernel estimator (for both we expect $\beta=-1/2$, i.e., ${\text RMSE}\propto n^{-1/2}$)

The following table shows the "empirical" rate of convergence ($\hat\beta$) in this case

```{r ordered}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
set.seed(42)

n.trials <- 2
n.vec <- c(50,100,200,400,800,1600,3200)
X.pred.dgp <- X.pred <- 0:2
X.pred <- ordered(X.pred)
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- rbinom(n.vec[j],n.trials,.5)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~ordered(X,levels=0:n.trials))
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))

for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.ot <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.ot) <- c("X","Oracle","NW","ratio")
```

```{r orderedtable}
knitr::kable(foo.ot,digits=2,align="rlll")
```

It appears that the kernel estimator attains the Oracle rate with **discrete regressors** and **data-driven smoothing parameter choice** with a **discrete support kernel function**

## Continuous Kernel, Relevant $X\in\mathbb{D}^1$

We consider $Y=\sin(2X)+\epsilon$, where $X\{0,1,2\}$

We implement the "Oracle" estimator and the kernel estimator (for both we expect $\beta=-1/2$, i.e., ${\text RMSE}\propto n^{-1/2}$)

The following table shows the "empirical" rate of convergence ($\hat\beta$) in this case

```{r bierens}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
set.seed(42)

n.trials <- 2
n.vec <- c(50,100,200,400,800,1600,3200)
X.pred.dgp <- X.pred <- 0:2
X.pred <- X.pred
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- rbinom(n.vec[j],n.trials,.5)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))

for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.bt <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.bt) <- c("X","Oracle","NW","ratio")
```

```{r bierenstable}
knitr::kable(foo.bt,digits=2,align="rlll")
```

It appears that the kernel estimator attains the Oracle rate with **discrete regressors** and **data-driven bandwidth choice** with a **continuous support kernel function**

## Continuous Kernel, Irrelevant $X\in\mathbb{R}^1$

We consider $Y=\sin(2X)+\epsilon$, where $X\sim U[-3,3]$

We implement the "Oracle" estimator and the kernel estimator (for the Oracle we expect $\beta=-1/2$, i.e., ${\text RMSE}\propto n^{-1/2}$, for the kernel we still expect $\beta=-1/2$, i.e., ${\text RMSE}\propto n^{-1/2}$)

The following table shows the "empirical" rate of convergence ($\hat\beta$) in this case

```{r irrelevant}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
## X.pred = c(-2,0,2) for a sample size of n = 50, 100, 200, 400, 800.
set.seed(42)

n.vec <- c(50,100,200,400,800,1600)
X.pred.dgp <- X.pred <- c(-2,0,2)
X.pred <- X.pred
dgp.pred <- rep(0,length(X.pred))
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- runif(n.vec[j],min=-3,max=3)
    dgp <- rep(0,length=n.vec[j])
    Y <- dgp + rnorm(n.vec[j],sd=.1)
    ghat.dgp <- lm(Y~1)
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.it <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.it) <- c("X","Oracle","NW","ratio")
```

```{r irrelevanttable}
knitr::kable(foo.it,digits=2,align="rlll")
```

It appears that the kernel estimator attains the Oracle rate with **irrelevant** **continuous regressors** and **data-driven bandwidth choice** with a **continuous support kernel function**

## Continuous Kernel, Relevant $X\in\mathbb{R}^1$

We consider $Y=\sin(2X)+\epsilon$, where $X\sim U[-3,3]$

We implement the "Oracle" estimator and the kernel estimator (for the Oracle we expect $\beta=-1/2$, i.e., ${\text RMSE}\propto n^{-1/2}$, for the kernel we expect $\beta=-2/5$, i.e., ${\text RMSE}\propto n^{-2/5}$)

The following table shows the "empirical" rate of convergence ($\hat\beta$) in this case

```{r continuous}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
## X.pred = c(-2,0,2) for a sample size of n = 50, 100, 200, 400, 800.
set.seed(42)

n.vec <- c(100,200,400,800,1600,3200,6400,12800)
X.pred.dgp <- X.pred <- c(-2,0,2)
X.pred <- X.pred
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- runif(n.vec[j],min=-3,max=3)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.ct <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.ct) <- c("X","Oracle","NW","ratio")
```

```{r continuoustable}
knitr::kable(foo.ct,digits=2,align="rlll")
```

It appears that the kernel rate of convergence is slower than the Oracle with **relevant** **continuous regressors** and **data-driven bandwidth choice** with a **continuous support kernel function**

## Simulations in KSZ-W (the "trinormal" case)

The authors state "The simulations demonstrate the effect of singularity on point-wise convergence rates for the kernel estimators." and that "This a.c. distribution represents features (high derivatives) that are more reflective of unsmooth densities we want to emphasize here."

```{r claw}
#| fig.asp=0.75
library(np)
library(robustbase)
options(np.tree=TRUE,np.messages=FALSE)
set.seed(42)

fx <- function(x) {
  0.5*dnorm(x+.767)+3*dnorm((x+.767-0.8)/.1)+2*dnorm((x+.767-1.2)/.1) 
}

fxmsd <- function(x) {
  0.5*dnorm(x,mean=-.767)+3*.1*dnorm(x,mean=-(.767-0.8),sd=.1)+2*.1*dnorm(x,mean=-(.767-1.2),sd=.1) 
}

rfxmsd <- function(n) {
  P <- sample(c("A","B","C"),n,replace=TRUE,prob=c(0.5,0.3,0.2))
  P.n <- numeric(n)
  P.n[P=="A"] <- rnorm(length(P[P=="A"]),mean=-.767)
  P.n[P=="B"] <- rnorm(length(P[P=="B"]),mean=-(.767-0.8),sd=.1)
  P.n[P=="C"] <- rnorm(length(P[P=="C"]),mean=-(.767-1.2),sd=.1) 
  return(P.n)
}

x <- seq(-3.5,2,length=1000)

par(mfrow=c(2,2))

n <- 10^4
breaks <- 100

X <- sort(rfxmsd(n))
xlim <- c(-3.5,2)
ylim <- range(hist(X,breaks=breaks,plot=FALSE)$density,fxmsd(x))

plot(x,fx(x),main="Density",xlim=xlim,xlab="X",ylab="fx(x)",type="l")
lines(x,fxmsd(x),lty=2,col=2)
abline(v=c(-.767,-(.767-0.8),-(.767-1.2)),col=2,lty=2)

hist(X,xlim=xlim,ylim=ylim,breaks=breaks,prob=TRUE,
     main="Histogram/Density",
     sub="Random Draw from fx(x)")
lines(x,fxmsd(x))
abline(v=c(-.767,-(.767-0.8),-(.767-1.2)),col=2,lty=2)

dgp <- sin(2.5*X)
Y <- dgp + rnorm(n,sd=sd(dgp))
plot(X,Y,cex=.1,xlim=xlim,col="grey")
ghat.dgp <- lm(Y~sin(2.5*X))
lines(X,fitted(ghat.dgp),col=2,lwd=2)
ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
lines(X,fitted(ghat.nw),col=3,lwd=3)
abline(v=c(-.767,-(.767-0.8),-(.767-1.2)),col=2,lty=2)
legend("topleft",c("Oracle","NW"),col=c(2,3),lwd=c(2,3),bty="n")

## Monte Carlo

n.vec <- c(50,100,200,400,800,1600,3200,6400,12800)
X.pred.dgp <- X.pred <- c(-.767,-(.767-0.8),-(.767-1.2))
X.pred <- X.pred
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- rfxmsd(n.vec[j])
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}

ylim <- range(c(rmse.dgp.pointwise,rmse.pointwise))

plot(X.pred.dgp,rmse.dgp.pointwise,main="Rate of Convergence",xlim=xlim,ylim=ylim,xlab="X",ylab="RMSE",type="b")
lines(X.pred.dgp,rmse.pointwise,xlab="X",ylab="RMSE",type="b",col="red")
abline(v=c(-.767,-(.767-0.8),-(.767-1.2)),col=2,lty=2)
legend("topleft",legend=c("Oracle","NW"),col=c("black","red"),lty=1,bty="n")

foo.claw <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.claw) <- c("X","Oracle","NW","ratio")
```

## Simulations in KSZ-W (the "trinormal" case)

The KSZ-W paper considers the case where $X$ is trinormal, but results are reported for a fixed $n$

The RMSE varies across the support for a fixed $n$, but although the authors say the simulations inform us about the rate of convergence, they do not report the RMSE *rate*

Perhaps results similar to that below would help (perhaps at the same uniform grid the authors use, or perhaps simply at a coarse grid of points

```{r clawtable}
knitr::kable(foo.claw,digits=2,align="rlll")
```

## Simulations in KSZ-W (the "trinormal" case)

To conduct the suggested exercise, the code is as follows:

```{r illustrative,echo=TRUE,eval=FALSE}
## Monte Carlo reps, vector of n, vector of X.pred
M <- 1000
n.vec <- c(50,100,200,400,800,1600,3200,6400,12800)
X.pred <- c(-.767,-(.767-0.8),-(.767-1.2))
## Prediction data frame
newdata <- data.frame(X=X.pred)
## Storage arrays
fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
## Monte Carlo
for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- rfxmsd(n.vec[j])
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}
## Compute pointwise RMSE at each support point for each n
rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}
## Compute RMSE rate
rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()
for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}

```

## References
