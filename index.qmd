---
title: "Kernel estimation in regression on vector and function spaces"
subtitle: "Sid Kankanala, Marcia Schafgans, and Victoria Zinde-Walsh"
author: Comments by Jeffrey Racine @ CESG 2023
date: today
format:
  revealjs:
    background-transition: fade
    center: true
    css: custom.css
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            tex: {
              tags: 'ams'
            }
          };
          </script>
    incremental: true
    link-external-newwindow: true
    ## multiplex: true will create two html files:
    ## 1. index.html: This is the file you should publish online and that your
    ## audience should view.
    ## 2. index-speaker.html: This is the file that you should present from.
    ## This file can remain on your computer and does not need to be published
    ## elsewhere.
    multiplex: true
    preview-links: auto
    self-contained-math: false
    show-notes: false
    slide-number: true
    theme: default
    touch: true
    transition: slide
knitr:
  opts_chunk:
    autodep: true
    collapse: true
    cache: true
    echo: false
    eval.after: "fig.cap"
    fig.align: "center"
    message: false
    warning: false
    R.options:
      np.messages: false
      plot.par.mfrow: false
bibliography: comments.bib      
editor: visual
---

```{r global_options}
#| include: false
library(robustbase)
library(np)
options(np.tree=TRUE,np.messages=FALSE)
M <- 1000
```

## Project Overview

-   The authors consider kernel-weighted nonparametric regression when the distribution of the regressors may not possess absolute continuity with respect to the Lebesgue measure

-   They demonstrate that convergence rates are influenced non-trivially by the underlying distribution

-   In the case of absolutely continuous measures, their approach weakens the usual regularity conditions

-   They extend their analysis to encompass kernel regression with multiple functional regressors

## Setting

-   The authors consider a smooth nonparametric model with additive errors $$Y=m(X)+u,\qquad E(u|X)=0$$

-   $X$ may belong to a function space, e.g., continuous functions on a compact set, or a more general metric space

-   $X$ does not necessarily have to be a vector or a function

-   $X$ is supported on some domain in a vector or metric, semi-metric space denoted as $\Xi$

-   The authors focus is on the generality of assumptions on the underlying distribution of $X$, i.e., on $F_X(x)$

::: notes
The fourteenth letter in the Greek alphabet, $\Xi$ is pronounced "kai"
:::

## Setting Cont.

-   The authors consider three cases:

    i.  $\Xi=\mathbb{R}^q$ (standard kernel regression)

    ii. $\Xi=\Xi^1$ (univariate metric space, i.e., functional regression)

    iii. $\Xi=\Xi^q=\Xi^1\times\cdots\times\Xi^1$ (product metric space)

-   The authors contribute to the literature by:

    i.  Establishing a novel CLT

    ii. Demonstrating that convergence rates are affected by the underlying distribution

    iii. Demonstrating that they can weaken usual regularity conditions assumed for such analysis

## What do we currently know ($X\in\mathbb{D}^1$)?

-   When $X$ has discrete support, kernel smoothing can attain a $\sqrt{n}$ rate for $E(Y|X=x)$

-   This is achieved by using, e.g., cross-validation to select smoothing parameters when using *categorical* kernel functions [@LI_RACINE:2007] or *continuous* kernel functions [@BIERENS:1983]

    ```{r categoricalkernel}
    #| out.width = "75%"
    n <- 100
    X <- sort(rbinom(n,5,.3))
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n,sd=sd(dgp))
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~sin(2.5*X))
    points(X,fitted(ghat.dgp),col=2)
    ghat.nw <- npreg(Y~ordered(X),bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

## What do we currently know ($X\in\mathbb{R}^1$)?

-   When $X$ has continuous support by is *unrelated* to $Y$, kernel smoothing can attain a $\sqrt{n}$ rate for $E(Y|X=x)=E(Y)$

-   This can be attained when using, e.g., cross-validation to select bandwidths that select $h\to\infty$ with high probability when using *continuous* kernel functions [@HALL_LI_RACINE:2007]

    ```{r continuouskernel}
    #| out.width = "75%"
    n <- 100
    X <- sort(runif(n))
    dgp <- rep(1,n)
    Y <- dgp + rnorm(n)
    plot(X,Y,cex=.5,col="grey")
    ghat.dgp <- lm(Y~1)
    points(X,fitted(ghat.dgp),col=2)
    ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
    points(X,fitted(ghat.nw),col=3)
    points(X,dgp,col=4)
    legend("topleft",c("Oracle","NP","DGP"),pch=1,col=2:4,bty="n")
    ```

## What do we currently know ($X\in\mathbb{R}^1$)?

-   Otherwise, when kernel smoothing under common regularity conditions, we attain a $\sqrt{nh}$ rate of convergence

-   Using an appropriate bandwidth, for $X\in\mathbb{R}^1$ ${\text RMSE}\propto n^{-2/5}$

-   The authors consider, instead, cases where the convergence rate, when using *continuous* kernel functions can be faster than the usual rate, for instance when there exist mass points in the support of $X$

## Assessing Pointwise Rates ($L^2$)

1.  The simulations are extensive but might be improved

2.  Bandwidth choice is ISE/IMSE optimal (is this wise? time for a rethink?)

3.  Perhaps the following might be an alternative to the exisitng simulations

    a.  run Monte Carlo simulations comparing various methods on predictions at a set of support points (as is currently done by the authors)

    b.  consider a range of sample sizes

    c.  for each replication record the pointwise RMSE

    d.  regress log(RMSE) on log(n) (the coefficient on log(n) will be the empirical rate of convergence, i.e., if ${\text RMSE} =\delta n^{-1/2}$ then $\log({\text RMSE}) =\alpha+\beta\log(n)$ and $\beta$ can be estimated)

## Catgorical $X\in\mathbb{D}^1$

Oracle & NW $\beta=1/2$

```{r ordered}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
set.seed(42)

n.trials <- 2
n.vec <- c(50,100,200,400,800,1600,3200)
X.pred.dgp <- X.pred <- 0:2
X.pred <- ordered(X.pred)
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- rbinom(n.vec[j],n.trials,.5)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~ordered(X,levels=0:n.trials))
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))

for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.ot <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.ot) <- c("X","Oracle","NW","ratio")
```

```{r orderedtable}
knitr::kable(foo.ot,digits=2,align="rlll")
```

## Continuous Kernel $X\in\mathbb{D}^1$

Oracle & NW $\beta=1/2$

```{r bierens}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
set.seed(42)

n.trials <- 2
n.vec <- c(50,100,200,400,800,1600,3200)
X.pred.dgp <- X.pred <- 0:2
X.pred <- X.pred
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- rbinom(n.vec[j],n.trials,.5)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))

for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.bt <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.bt) <- c("X","Oracle","NW","ratio")
```

```{r bierenstable}
knitr::kable(foo.bt,digits=2,align="rlll")
```

## Irrelevant $X\in\mathbb{R}^1$

Oracle & NW $\beta=1/2$

```{r irrelevant}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
## X.pred = c(-2,0,2) for a sample size of n = 50, 100, 200, 400, 800.
set.seed(42)

n.vec <- c(50,100,200,400,800,1600)
X.pred.dgp <- X.pred <- c(-2,0,2)
X.pred <- X.pred
dgp.pred <- rep(0,length(X.pred))
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- runif(n.vec[j],min=-3,max=3)
    dgp <- rep(0,length=n.vec[j])
    Y <- dgp + rnorm(n.vec[j],sd=.1)
    ghat.dgp <- lm(Y~1)
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.it <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.it) <- c("X","Oracle","NW","ratio")
```

```{r irrelevanttable}
knitr::kable(foo.it,digits=2,align="rlll")
```

## Relevant $X\in\mathbb{R}^1$

Oracle $\beta=-1/2$, NW $\beta=-2/5$

```{r continuous}
## Write a monte carlo simulation that compute the pointwise RMSE of the fitted
## values generated by npreg() using the newdata evaluation points
## X.pred = c(-2,0,2) for a sample size of n = 50, 100, 200, 400, 800.
set.seed(42)

n.vec <- c(100,200,400,800,1600,3200,6400,12800)
X.pred.dgp <- X.pred <- c(-2,0,2)
X.pred <- X.pred
dgp.pred <- sin(2.5*X.pred.dgp)
newdata.dgp <- data.frame(X=X.pred.dgp)
newdata <- data.frame(X=X.pred)

fitted.array.dgp <- array(NA,dim=c(length(n.vec),M,length(X.pred)))
fitted.array <- array(NA,dim=c(length(n.vec),M,length(X.pred)))

for(j in 1:length(n.vec)) {
  for(i in 1:M) {
    X <- runif(n.vec[j],min=-3,max=3)
    dgp <- sin(2.5*X)
    Y <- dgp + rnorm(n.vec[j],sd=sd(dgp))
    ghat.dgp <- lm(Y~sin(2.5*X))
    fitted.array.dgp[j,i,] <- predict(ghat.dgp,newdata=newdata.dgp)
    ghat.nw <- npreg(Y~X,ckertype="epanechnikov")
    fitted.array[j,i,] <- predict(ghat.nw,newdata=newdata)
  }
}

## Compute pointwise RMSE at each support point

rmse.dgp <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
rmse <- matrix(NA,nrow=length(n.vec),ncol=length(X.pred))
for(j in 1:length(n.vec)) {
  rmse.dgp[j,] <- sqrt(colMeans(sweep(fitted.array.dgp[j,,],2,dgp.pred)^2))
  rmse[j,] <- sqrt(colMeans(sweep(fitted.array[j,,],2,dgp.pred)^2))
}

rmse.dgp.pointwise <- numeric()
rmse.pointwise <- numeric()

for(i in 1:length(X.pred)) {
  rmse.dgp.pointwise[i] <- coef(ltsReg(log(rmse.dgp[,i])~log(n.vec)))[2]
  rmse.pointwise[i] <- coef(ltsReg(log(rmse[,i])~log(n.vec)))[2]
}
foo.ct <- data.frame(X.pred,rmse.dgp.pointwise,rmse.pointwise,rmse.pointwise/rmse.dgp.pointwise)
colnames(foo.ct) <- c("X","Oracle","NW","ratio")
```

```{r continuoustable}
knitr::kable(foo.ct,digits=2,align="rlll")
```

## Comments

```{r comments}
#| fig.asp=0.75
set.seed(42)

fx <- function(x) {
  0.5*dnorm(x+.767)+3*dnorm((x+.767-0.8)/.1)+2*dnorm((x+.767-1.2)/.1) 
}

fxmsd <- function(x) {
  0.5*dnorm(x,mean=-.767)+3*.1*dnorm(x,mean=-(.767-0.8),sd=.1)+2*.1*dnorm(x,mean=-(.767-1.2),sd=.1) 
}

rfxmsd <- function(n) {
  P <- sample(c("A","B","C"),n,replace=TRUE,prob=c(0.5,0.3,0.2))
  P.n <- numeric(n)
  P.n[P=="A"] <- rnorm(length(P[P=="A"]),mean=-.767)
  P.n[P=="B"] <- rnorm(length(P[P=="B"]),mean=-(.767-0.8),sd=.1)
  P.n[P=="C"] <- rnorm(length(P[P=="C"]),mean=-(.767-1.2),sd=.1) 
  return(P.n)
}

x <- seq(-3.5,2,length=1000)

par(mfrow=c(2,2))

n <- 10^4
breaks <- 100

X <- sort(rfxmsd(n))
xlim <- c(-3.5,2)
ylim <- range(hist(X,breaks=breaks,plot=FALSE)$density,fxmsd(x))

plot(x,fx(x),main="Density",xlim=xlim,xlab="X",ylab="fx(x)",type="l")
lines(x,fxmsd(x),lty=2,col=2)

hist(X,xlim=xlim,ylim=ylim,breaks=breaks,prob=TRUE,
     main="Histogram/Density",
     sub="Random Draw from fx(x)")
lines(x,fxmsd(x))

dgp <- sin(2.5*X)
Y <- dgp + rnorm(n,sd=sd(dgp))
plot(X,Y,cex=.1,xlim=xlim,col="grey")
ghat.dgp <- lm(Y~sin(2.5*X))
lines(X,fitted(ghat.dgp),col=2,lwd=2)
ghat.nw <- npreg(Y~X,bwtype="fixed",ckertype="epanechnikov")
lines(X,fitted(ghat.nw),col=3,lwd=3)
legend("topleft",c("Oracle","NW"),col=c(2,3),lwd=c(2,3),bty="n")
```

## Comments

-   Figures - mixing $f(x)$ on vertical axis and $F(x)$ in description?

## References
